{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hong Kong Subreddit Data 2010-2018 - Pushshift API (Part 1)\n",
    "HongKong subreddit first started in Nov, 2009.\n",
    "\n",
    "There are a number of ways to obtain reddit data:\n",
    "- Web scraping using packages such as beautifulsoup, selenium, \n",
    "- Reddit API using PRAW\n",
    "- Third Party API such as PushShift.io\n",
    "\n",
    "The first method is requires a large amount of time to collect 8 years worth of submissions (posts created by users) and comments.\n",
    "The second method has a limitation - 1000 lastest submissions/comments only.\n",
    "Given the constraints, I chose to go with the third method. More information on Pushshift can be found [here](https://pushshift.io/author/stuck_in_the_matrix/).\n",
    "\n",
    "Note: Pushshift's API documentation is not well up-to-date so browsing the pushshift subreddit may help solve your issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushshift.io provides an API that allows users to query submissions and comments of reddit. Submissions and comments data are separated into different sections, so it will need to run the function twice. \n",
    "\n",
    "Although each query provides a maximum of 1000 posts, it can search for comments days after/before the current date. If I want to obtain data from 1 year ago until now, you would add 365d in the after parameter of the API. \n",
    "\n",
    "Since our aim is to obtain 8 years of data, the script below will collect 1000 posts for each iteration and the next query will slightly overlap the previous posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_data(start_date_epoch, data_type):\n",
    "    \"\"\"\n",
    "    Obtain all HongKong subreddit data from a start date until today.\n",
    "\n",
    "    :param start_date_epoch: Epoch timestamp format. If you provide epoch in milliseconds divide by 1000.\n",
    "    :param data_type: String Type. data_type is comment or submission\n",
    "    :returns: List of data_type\n",
    "    \"\"\"\n",
    "    end_time = datetime.datetime.fromtimestamp(round(datetime.datetime.now().timestamp()))\n",
    "    start_time = datetime.datetime.fromtimestamp(round(start_date_epoch))\n",
    "    remaining_days = (end_time-start_time).days\n",
    "    data = []\n",
    "    \n",
    "    while remaining_days > 0:\n",
    "    \n",
    "        #size = max number of retrievials\n",
    "        #after = search posts after # days\n",
    "        response = requests.get('https://api.pushshift.io/reddit/search/{}/?subreddit=HongKong&size=1000&after={}d'\\\n",
    "                                .format(data_type, str(remaining_days))) \n",
    "        json = response.json()\n",
    "        data.extend(json['data'])\n",
    "        \n",
    "        last_post_time = json['data'][-1]['created_utc']\n",
    "        start_time = datetime.datetime.fromtimestamp(round(last_post_time))\n",
    "        remaining_days = (end_time-start_time).days\n",
    "        \n",
    "        #minimum delay required as requested by pushshift to prevent hitting servers too hard\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a list of raw submissions and comments data\n",
    "- Obtaining comments data may take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain submissions data starting from Jan, 2010\n",
    "submissions = get_reddit_data(1262304000, 'submission')\n",
    "#Json format\n",
    "json_data_submissions ={'data':submissions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output submissions data into file as a checkpoint\n",
    "with open('hongkong_submissions.json', 'w') as outfile:  \n",
    "    json.dump(json_data_submissions, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain comments data starting from Jan, 2010\n",
    "comments = get_reddit_data(1262304000, 'comment')\n",
    "#Json format\n",
    "json_data_comments ={'data':comments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output comments data into file as a checkpoint\n",
    "with open('hongkong_comments.json', 'w') as outfile:  \n",
    "    json.dump(json_data_comments, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicates from the raw comments and submissions data\n",
    "- Since calling the API introduced some overlapping, we will need to de-duplicate the comments and submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_list = submissions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_id_list(data_type_list):\n",
    "    \"\"\"\n",
    "    Gets a list of duplicate items by id\n",
    "    \n",
    "    :param data_type_list: A list of submissions/comments.\n",
    "    :returns: List of duplicated items within data_type_list\n",
    "    \"\"\"\n",
    "    duplicate_ids = []\n",
    "    \n",
    "    for i in range(len(data_type_list)-1):\n",
    "        data_id = data_type_list[i]['id']\n",
    "        start_time = datetime.datetime.fromtimestamp(round(data_type_list[i]['created_utc']))\n",
    "        \n",
    "        for j in range(i+1,len(data_type_list)):\n",
    "            end_time = datetime.datetime.fromtimestamp(round(data_type_list[j]['created_utc']))\n",
    "            difference_days = (end_time-start_time).days\n",
    "            \n",
    "            if data_id == data_type_list[j]['id']: # if duplicate add to list\n",
    "                duplicate_ids.append(data_id)\n",
    "                break\n",
    "            elif difference_days > 2: #break if cannot find duplicate after 2 days of post\n",
    "                break\n",
    "                \n",
    "    return duplicate_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Gets a list of duplicate items\n",
    "\"\"\"\n",
    "def get_duplicate_id_list(submission_list):\n",
    "    duplicate_ids = []\n",
    "    for i in range(len(submission_list)-1):\n",
    "        submission_id = submission_list[i]['id']\n",
    "        for j in range(i+1,len(submission_list)):\n",
    "            if submission_id == submission_list[j]['id']:\n",
    "                duplicate_ids.append(submission_id)\n",
    "                break\n",
    "    return duplicate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data_type_list):\n",
    "    \"\"\"\n",
    "    Removes duplicate items from data_type_list\n",
    "    \n",
    "    :param data_type_list: A list of submissions/comments.\n",
    "    :returns: List of de-duplicated data_type_list\n",
    "    \"\"\"\n",
    "    duplicate_ids = get_duplicate_id_list(data_type_list)\n",
    "    \n",
    "    for ids in duplicate_ids:\n",
    "        for index, element in reversed(list(enumerate(data_type_list))):\n",
    "            if element['id'] == ids: # find and remove the first duplicate from the back of the list\n",
    "                data_type_list.pop(index)\n",
    "                break\n",
    "                \n",
    "    return data_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_submission_list = remove_duplicates(submission_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json format\n",
    "json_data_submission_filtered ={'data':filtered_submission_list}\n",
    "# output submissions data into file as a checkpoint\n",
    "with open('hongkong_submissions_filtered.json', 'w') as outfile:  \n",
    "    json.dump(json_data_submission_filtered, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = comments.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_comments_list = remove_duplicates(comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json format\n",
    "json_data_comments_filtered ={'data':filtered_comments_list}\n",
    "# output comments data into file as a checkpoint\n",
    "with open('hongkong_comments_filtered.json', 'w') as outfile:  \n",
    "    json.dump(json_data_comments_filtered, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining comments to their submission posts.\n",
    "\n",
    "Submission ids and comments ids are matched by id and linked_id respectively. \n",
    "\n",
    "In order to efficiently match submissions and comments, comments are grouped together. Then iterate through the submission list and insert the relevant grouped comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_comments(json_comments):\n",
    "    \"\"\"\n",
    "    Combine comments that share the same submission id\n",
    "    \n",
    "    :param data_type_list: A list of comments\n",
    "    :returns: Dictionary. key: submission id, value: list of comments\n",
    "    \"\"\"\n",
    "    submission_record = {}\n",
    "    \n",
    "    for i in range(len(json_comments)):\n",
    "        if json_comments[i]['link_id'][3:] not in submission_record: # create new key if id doesn't exist in submission_record\n",
    "            comments = [json_comments[i]]\n",
    "            comment_first_date = datetime.datetime.fromtimestamp(round(json_comments[i]['created_utc']))\n",
    "            \n",
    "            for j in range(i+1,len(json_comments)):\n",
    "                #if id matches key in submission_record, add it.\n",
    "                if json_comments[j]['link_id'][3:] == json_comments[i]['link_id'][3:]: \n",
    "                    comments.append(json_comments[j])\n",
    "                else:\n",
    "                    # if id doesn't match and if post is greater than 60 days old, stop the search.\n",
    "                    comment_latest_date = datetime.datetime.fromtimestamp(round(json_comments[j]['created_utc']))\n",
    "                    remaining_days = (comment_latest_date-comment_first_date).days\n",
    "                    if remaining_days > 60:\n",
    "                        break\n",
    "            # add list of comments to submission_record key (id)        \n",
    "            submission_record[json_comments[i]['link_id'][3:]] = comments\n",
    "            \n",
    "    return submission_record "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_comments = combine_comments(filtered_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output grouped comments data into file as a checkpoint\n",
    "with open('hongkong_combined_comments.json', 'w') as outfile:  \n",
    "    json.dump(dict_comments, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_comments_submission(submission_list, dict_comments):\n",
    "    \"\"\"\n",
    "    Insert grouped comments to submission list by id\n",
    "    \n",
    "    :param submission_list: A list of submissions\n",
    "    :param dict_comments: A dictionary of comments\n",
    "    :returns: A list of submissions\n",
    "    \"\"\"\n",
    "    submissions = submission_list.copy()\n",
    "    for submission in submissions:\n",
    "        if submission['id'] in dict_comments:\n",
    "            submission['comments'] = dict_comments[submission['id']]\n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_json = insert_comments_submission(filtered_submission_list, dict_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete processing output to json file\n",
    "with open('hongkong_complete.json', 'w') as outfile:  \n",
    "    json.dump(complete_json, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
