{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hong Kong Reddit - N-gram Comments Processing (Part 2)\n",
    "\n",
    "After Part 1, we will process the comments data to produce ngrams for the ngram viewer. \n",
    "\n",
    "What are ngrams?\n",
    "A sequence of n words (where n is a postive number). An example sentence is: I love Hong Kong.\n",
    "* Unigram consist of a single word. e.g. I, love, Hong, Kong\n",
    "* Bigrams consist of two words. e.g. I love, love Hong, Hong Kong\n",
    "* Trigrams consist of three words. e.g. I love Hong, love Hong Kong.\n",
    "\n",
    "We will only generate up to 3 ngrams as larger ngrams:\n",
    "1. Require more storage and RAM to process\n",
    "2. Hard to find non-unique 5 gram words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hongkong_comments_filter.json', 'r') as f:  \n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize english stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores all comments that are in the same year\n",
    "data = {\n",
    "    2010:[],\n",
    "    2011:[],\n",
    "    2012:[],\n",
    "    2013:[],\n",
    "    2014:[],\n",
    "    2015:[],\n",
    "    2016:[],\n",
    "    2017:[],\n",
    "    2018:[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments['data']:\n",
    "    comment_year = datetime.datetime.fromtimestamp(round(comment['created_utc'])).year\n",
    "    \n",
    "    if comment['body'] != '[deleted]': # skip deleted comments\n",
    "        result = re.sub(r\"http\\S+|Http\\S+|&gt;\", \"\", comment['body']) #remove http links and &gt; artefacts in the data\n",
    "        data[comment_year].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer. Tokenize words and numbers only. No punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# For each year...\n",
    "for key in data:\n",
    "    # lower case all elements then join then together\n",
    "    data[key] = (' '.join(filter(None, data[key]))).lower()\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(data[key])\n",
    "    # filter out stopwords from tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # remove amp\n",
    "    data[key] = [t for t in tokens if t not in ['amp']] #http?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the words\n",
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store total frequency of words in each year\n",
    "total_word = {\n",
    "    2010: 0,\n",
    "    2011: 0,\n",
    "    2012: 0,\n",
    "    2013: 0,\n",
    "    2014: 0,\n",
    "    2015: 0,\n",
    "    2016: 0,\n",
    "    2017: 0,\n",
    "    2018: 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year...\n",
    "for key in data:\n",
    "    # for ngram 1 to 3\n",
    "    for i in range(1,4):\n",
    "        # generate ngrams and put into a list\n",
    "        bi_grams = list(ngrams(data[key], i))\n",
    "        # Count the number of words\n",
    "        counter = Counter(bi_grams)\n",
    "        \n",
    "        # for each word...\n",
    "        for word in counter.most_common():\n",
    "            combine_word = ' '.join(word[0]) #combine bigrams, trigrams\n",
    "            \n",
    "            # create new key (word) in words dictionary if not exist\n",
    "            if combine_word not in words:\n",
    "                words[combine_word] = [{2010 : 0, 2011 : 0, 2012 : 0, 2013 : 0, 2014 : 0, 2015 : 0, 2016 : 0, \n",
    "                                        2017 : 0, 2018 : 0},]\n",
    "            \n",
    "            # add value for word\n",
    "            words[combine_word][0][key] = word[1]\n",
    "\n",
    "# for each word...\n",
    "for word in words:\n",
    "    total_count = 0\n",
    "    # for each year and value of the word...\n",
    "    for year, year_value in words[word][0].items():\n",
    "        total_count += year_value # add up for total frequency of the word in the year\n",
    "        total_word[year] += year_value # add all total frequency words in the year to calculate relative frequency\n",
    "    words[word].append(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative frequency of the words in each year\n",
    "for word in words:\n",
    "    for year, year_value in words[word][0].items():\n",
    "        words[word][0][year] = year_value / total_word[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words.pop(2010, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format to be uploaded to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for SQL upload. Not advisable due to 1.2 GB size.\n",
    "with open('ngram_data.sql', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"'{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\"'+ str(year_key) + '\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = \"INSERT INTO jobs_ngram_hk (id,word,json,frequency) VALUES (\" + str(table_id) + \",'\" + str(key) + \n",
    "                    \"',\" + json_str + \"}',\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \");\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for client postgres upload. Delimiter using ;\n",
    "with open('ngram_data.csv', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"\\\"{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\\'\\\"'+ str(year_key) + '\\'\\\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = str(table_id) + \";\" + str(key) + \";\" + json_str + \"}\\\";\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for /copy upload using psql. Delimiter using ;\n",
    "with open('ngram_data.csv', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"\\\"{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\\\\\"'+ str(year_key) + '\\\\\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = str(table_id) + \";\" + str(key) + \";\" + json_str + \"}\\\";\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
