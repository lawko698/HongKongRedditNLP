{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hong Kong Reddit - N-gram Comments Processing (Part 2)\n",
    "\n",
    "After Part 1, we will process the comments data to produce ngrams for the ngram viewer. \n",
    "\n",
    "What are ngrams?\n",
    "A sequence of n words (where n is a postive number). An example sentence is: I love Hong Kong.\n",
    "* Unigram consist of a single word. e.g. I, love, Hong, Kong\n",
    "* Bigrams consist of two words. e.g. I love, love Hong, Hong Kong\n",
    "* Trigrams consist of three words. e.g. I love Hong, love Hong Kong.\n",
    "\n",
    "We will only generate up to 3 ngrams as larger ngrams:\n",
    "1. Require more storage and RAM to process\n",
    "2. Hard to find non-unique 5 gram words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hongkong_comments_filtered.json', 'r') as f:  \n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize english stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores all comments that are in the same year\n",
    "data = {\n",
    "    2010:[],\n",
    "    2011:[],\n",
    "    2012:[],\n",
    "    2013:[],\n",
    "    2014:[],\n",
    "    2015:[],\n",
    "    2016:[],\n",
    "    2017:[],\n",
    "    2018:[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments['data']:\n",
    "    comment_year = datetime.datetime.fromtimestamp(round(comment['created_utc'])).year\n",
    "    \n",
    "    if comment['body'] != '[deleted]': # skip deleted comments\n",
    "        result = re.sub(r\"http\\S+|Http\\S+|&gt;\", \"\", comment['body']) #remove http links and &gt; artefacts in the data\n",
    "        data[comment_year].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer. Tokenize words and numbers only. No punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# For each year...\n",
    "for key in data:\n",
    "    # lower case all elements then join then together\n",
    "    data[key] = (' '.join(filter(None, data[key]))).lower()\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(data[key])\n",
    "    # filter out stopwords from tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # remove amp\n",
    "    data[key] = [t for t in tokens if t not in ['amp']] #http?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the words\n",
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store total frequency of words in each year\n",
    "total_word = {\n",
    "    2010: 0,\n",
    "    2011: 0,\n",
    "    2012: 0,\n",
    "    2013: 0,\n",
    "    2014: 0,\n",
    "    2015: 0,\n",
    "    2016: 0,\n",
    "    2017: 0,\n",
    "    2018: 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year...\n",
    "for key in data:\n",
    "    # for ngram 1 to 3\n",
    "    for i in range(1,4):\n",
    "        # generate ngrams and put into a list\n",
    "        bi_grams = list(ngrams(data[key], i))\n",
    "        # Count the number of words\n",
    "        counter = Counter(bi_grams)\n",
    "        \n",
    "        # for each word...\n",
    "        for word in counter.most_common():\n",
    "            combine_word = ' '.join(word[0]) #combine bigrams, trigrams\n",
    "            \n",
    "            # create new key (word) in words dictionary if not exist\n",
    "            if combine_word not in words:\n",
    "                words[combine_word] = [{2010 : 0, 2011 : 0, 2012 : 0, 2013 : 0, 2014 : 0, 2015 : 0, 2016 : 0, \n",
    "                                        2017 : 0, 2018 : 0},]\n",
    "            \n",
    "            # add value for word\n",
    "            words[combine_word][0][key] = word[1]\n",
    "\n",
    "# for each word...\n",
    "for word in words:\n",
    "    total_count = 0\n",
    "    # for each year and value of the word...\n",
    "    for year, year_value in words[word][0].items():\n",
    "        total_count += year_value # add up for total frequency of the word in the year\n",
    "        total_word[year] += year_value # add all total frequency words in the year to calculate relative frequency\n",
    "    words[word].append(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequency\n",
    "\n",
    "Since absolute frequency does not provide interesting information relative to the other words used during that time, we will use relative frequency. \n",
    "\n",
    "For example, the term 'Occupy central' only emerged in 2014 during protest that occured in Central, Hong Kong. This term may have been cited frequently over time in absolute terms, however, in terms of relative frequency, it is cited less compared to the word 'independence' after a year or two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative frequency of the words in each year\n",
    "for word in words:\n",
    "    for year, year_value in words[word][0].items():\n",
    "        words[word][0][year] = year_value / total_word[year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format to be uploaded to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for SQL upload. Not advisable due to 1.2 GB size.\n",
    "with open('ngram_data.sql', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"'{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\"'+ str(year_key) + '\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = \"INSERT INTO jobs_ngram_hk (id,word,json,frequency) VALUES (\" + str(table_id) + \",'\" + str(key) + \n",
    "                    \"',\" + json_str + \"}',\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \");\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for client postgres upload. Delimiter using ;\n",
    "with open('ngram_data.csv', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"\\\"{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\\'\\\"'+ str(year_key) + '\\'\\\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = str(table_id) + \";\" + str(key) + \";\" + json_str + \"}\\\";\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for /copy upload using psql. Delimiter using ;\n",
    "with open('ngram_data.csv', 'w', encoding=\"utf-8\") as outfile: \n",
    "    table_id = 1\n",
    "    for key in words:\n",
    "        json_str = \"\\\"{\"\n",
    "        for year_key in words[key][0]:\n",
    "            string = '\\\\\"'+ str(year_key) + '\\\\\":' + str(words[key][0][year_key])\n",
    "            if year_key == 2018:\n",
    "                json_str = json_str + string\n",
    "            else:\n",
    "                json_str = json_str + string + ','\n",
    "        finalstr = str(table_id) + \";\" + str(key) + \";\" + json_str + \"}\\\";\" + str(words[key][1])\n",
    "        table_id += 1\n",
    "        outfile.write(finalstr + \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
